{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test artifacts are the different and many products created during the software testing, just like test strategies, test cases, test scenarios, log files, etc. The idea is to share these artifacts with the team and stakeholders, so that there’s no miscommunication between them and the test scope is well understood. Usually, the source code for the tests are also referred to as artifacts.\n",
    "\n",
    "As you begin to write tests in your projects, you'll notice that the amount of test code written and maintained by a a software development team is quite significant. In practice, test code bases tend to grow quickly. Empirically, we have been observing that Lehman's laws of evolution also apply to test code: code tends to rot, unless one actively works against it. Therefore, as with production code, developers have to put extra effort into making high-quality test code bases, so that these can be maintained and developed in a sustainable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five basic rules that every test should obey to be considered a good — or even a valid — test. As defined in Clean Code method [1], the mnemonic for these rules is: F.I.R.S.T., pointing out that the tests shoud be:\n",
    "- Fast — tests should execute quickly so the test suite can be executed often.\n",
    "- Isolated — tests on their own cannot depend on external factors or on the result of another test.\n",
    "- Repeatable — tests should have the same result every time we run them.\n",
    "- Self-verifying — tests should include assertions; no human intervention needed.\n",
    "- Timely — tests should be written along with the production code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On\n",
    "\n",
    "Check the code below and find out which practices are not recommended for testing cases artifacts. If you're stuck, jump to the next section, where we go through some common bad practices.\n",
    "<a href=\"https://colab.research.google.com/github/damorimRG/practical_testing_book/blob/master/goodpractices/artifacts.ipynb\" target=\"_blank\"> \n",
    "    <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestFlight(unittest.TestCase):    \n",
    "    def test_bad_practices(self):\n",
    "        self.assertTrue(Flight('2569',1000).isValidAirLineCode())\n",
    "        self.assertTrue(Flight('2569',1000).fullFuel)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common bad practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll dive into some bad practices, with a few examples that break the previous suggested rules. Then, we'll see how to deal with those practices to create better test artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic Number Test\n",
    "\n",
    "In this example, `calculateBmi` is a function that calculates the body mass index (BMI) of a person. The tests are working, but, as we'll see later, there's something missing on this test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    " \n",
    "def calculateBmi(weight,height):\n",
    "    bmi = weight/(height * height)\n",
    "    \n",
    "    return round(bmi,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestCalculateBmi(unittest.TestCase):\n",
    "    \n",
    "    def test_calculate_Bmi(self):\n",
    "        self.assertEqual(calculateBmi(80,1.70),27.68)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test method, the significance of the numeric literals that was passed as parameter in the assertion method is unknowed, so we should modify it to clarify the meaning of this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestCalculateBmi(unittest.TestCase):\n",
    "    def test_calculate_Bmi(self):\n",
    "       bmi = 27.68\n",
    "       weight = 80\n",
    "       height = 1.70\n",
    "       self.assertEqual(calculateBmi(weight,height),bmi)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "   unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assertion Roulette Test\n",
    "Diving a little deeper, the next example is a bit more complex. This time, the class `Flight` is the system under test. The test case `test_flight` has three assertions, making it hard to tell which one may cause a test failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    " \n",
    "airLinesCode = ['2569','2450','2340']\n",
    " \n",
    "class Flight:\n",
    "    def __init__(self,airLine,mileage):\n",
    "        self.mileage = mileage\n",
    "        self.airLine = airLine\n",
    "        self.fullFuel = True\n",
    "        \n",
    "    def isValidAirLineCode(self):\n",
    "        for airLineCode in airLinesCode:\n",
    "            if(self.airLine == airLineCode):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestFlight(unittest.TestCase):\n",
    "    def test_flight(self):\n",
    "        flight = Flight('2569',1000)\n",
    "        \n",
    "        self.assertEqual(flight.mileage,1000)\n",
    "        self.assertTrue(flight.fullFuel)\n",
    "        self.assertTrue(flight.isValidAirLineCode())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution for this case is to divide this function into multiple tests, making sure that there's only one assertion per test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestFlight(unittest.TestCase):\n",
    "    def test_mileage_init(self):\n",
    "        airLine = '2569'\n",
    "        mileage = 1000\n",
    "        flight = Flight(airLine,mileage)\n",
    "        self.assertEqual(flight.mileage,1000)\n",
    "        \n",
    "    def test_fuel_is_full(self):\n",
    "        airLine = '2569'\n",
    "        mileage = 1000\n",
    "        flight = Flight(airLine,mileage)\n",
    "        self.assertTrue(flight.fullFuel)\n",
    "        \n",
    "    def test_is_valid_air_line_code(self):\n",
    "        airLine = '2569'\n",
    "        mileage = 1000\n",
    "        flight = Flight(airLine,mileage)\n",
    "        self.assertTrue(flight.isValidAirLineCode())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Code Duplication\n",
    "It's very common that more than one test in a suite needs to be setup or do similar computations. For instance, in the example above, the `flight` object is created in every test. Therefore, we can refactor the suite, creating a function that sets up the variables used for most tests - in this case, the `flight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestFlight(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        airLine = '2569'\n",
    "        mileage = 1000\n",
    "        self.flight = Flight(airLine,mileage)\n",
    "        \n",
    "    def test_mileage_init(self):\n",
    "        mileage = 1000\n",
    "        self.assertEqual(self.flight.mileage,mileage)\n",
    "    \n",
    "    def test_fuel_is_full(self):\n",
    "        self.assertTrue(self.flight.fullFuel)\n",
    "        \n",
    "    def test_is_valid_air_line_code(self):\n",
    "        self.assertTrue(self.flight.isValidAirLineCode())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have explored some good practices to enhance the testing projects for better quality and security of your software. Having experienced and expert software testing development practices is mandatory to hit your testing goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [1] Robert C. Martin, Clean Code: A Handbook of Agile Software Craftsmanship\n",
    "- [Luis Solano, objc.io, August 2014, Bad Testing Practices](https://www.objc.io/issues/15-testing/bad-testing-practices/#good-practices-101)\n",
    "- [RIT, testsmells.github.io, Test Smell Examples](https://testsmells.github.io/pages/testsmellexamples.html)\n",
    "- [Maurício Aniche, Software Testing: From Theory to Practice](https://sttp.site/)\n",
    "- [Gerard Meszaros, xUnit Patterns](http://xunitpatterns.com/Assertion%20Roulette.html)\n",
    "- [Wideskills, Test Artifacts](https://www.wideskills.com/software-testing-tutorial/test-artifacts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
